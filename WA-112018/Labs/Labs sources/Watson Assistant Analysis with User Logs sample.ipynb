{"nbformat_minor": 2, "cells": [{"source": "<div style=\"background:#F5F7FA; height:100px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Want to do more?</span><span style=\"border: 1px solid #3d70b2;padding: 15px;float:right;margin-right:40px; color:#3d70b2; \"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n<span style=\"color:#5A6872;\"> Try out this notebook with your free trial of IBM Watson Studio.</span>\n</div>", "cell_type": "markdown", "metadata": {}}, {"source": "# Watson Assistant Workspace Analysis with User Logs\n\n\nThis guide demonstrates how to combine a Watson Data Notebook with Watson Assistant to analyze and improve a conversational application through establishing a baseline of performance and then creating, running and visualizing the results of experiments.  The pre-requisites for running this notebook are [IBM Cloudant](https://www.ibm.com/cloud/cloudant) and [Watson Assistant (formerly Watson Conversation).](http://www.ibm.com/watson/developercloud/conversation.html)  This notebook assumes familiarity with Watson Assistant and concepts such as workspaces, intents and training examples. For more information on getting started with the Watson Assistant, check out the [Watson Developer Cloud.](http://www.ibm.com/watson/developercloud/conversation.html)  Free plans for Watson Assistant and Cloudant are available and will work to get started with this notebook. \n\nDuring development of this notebook, the authors used the Car Dashboard sample workspace available to every Watson Assistant instance.  Other workspaces can be used instead.\n\n### <span style=\"color:red\">Key points about data usage: \n* This notebook will retrieve workspace data and log data from the Watson Assistant instance you configure in the notebook. \n* You will need to have two Assistant workspaces. One workspace should represent your production/baseline that you want to improve upon.  A second workspace should represent a test/experimental workspace that you will change and compare against your baseline.\n* The results of extracting logs from your baseline workspace, and results from calling your test/experiment workspace will be written to a Cloudant instance that you configure. If you prefer to store data elsewhere, you should modify the notebook steps that reference Cloudant.\n* If you want to first try this notebook on sample data, you can use the Car Dashboard sample that is available with Watson Assistant. Simply use the Assistant User interface to create two copies of the Car Dashboard workspace, and generate user log data with the copy that you designate as your production/baseline workspace. To generate user log data, you can use a script like [generate_chat_logs](https://github.com/watson-developer-cloud/community/tree/master/watson-assistant#generate-chat-logs) and the sample [utterances-for-generate-chat-logs CSV file](https://github.com/watson-developer-cloud/community/blob/master/watson-assistant/utterances-for-generate-chat-logs.csv) available for download from the [Watson Community Github](https://github.com/watson-developer-cloud/community/tree/master/watson-assistant) repo. \n\nThis notebook runs on Python 3.5 with Spark 2.1.\n\n## Table of Contents\n\nThis notebook is organized in four major sections. First, we setup the notebook with library imports and connections to Watson Assistant and Cloudant.  Then we'll define a set of supporting Python functions. Then we'll run the functions to gather data. Finally we'll graph the results for analysis and look for areas of improvement.   \n________\n\n1. [Install and import packages](#setup) \n2. [Prompt for Cloudant and Watson Assistant credentials](#credentials)\n3. [Create the Cloudant database](#cloudant)\n4. [Connect to Watson Assistant](#connect_assistant)<br>\nNext, define a set of Python functions to interact with Cloudant and Watson Assistant.\n5. [Get utterances from log data and store it in Cloudant for later use.](#get_logs)\n6. [Send experiment questions to Test workspace](#send_questions)\n7. [Create and Store Baseline ](#create_and_store_baseline)\n8. [Get baselines from Cloudant ](#get_baselines)\n9. [Define a function to run a specific experiment using baseline name](#run_experiment)\n10. [Get previous experiments for a specific baseline](#get_experiments_for_baseline)\n11. [Compute/Compile stats for a workspace ](#compile_workspace_stats)<br>\nFunction definitions end here. Now we'll run the functions to gather data. \n12. [Call the function to create and store the baseline ](#call_create_store_baseline)\n13. [Call function to run experiment and store results ](#get_baseline_run_experiment)\n14. [Compile workspace stats for baseline and experiment](#call_compile_workspace_stats)\n15. [Prepare workspace analysis data and display the Intent distribution](#prepare_frames_display)<br>\nNow we'll graph the results for analysis and look for areas of improvement.\n16. [Compare the baseline vs. experiment workspaces](#display_ws_analysis)\n17. [Display number of training examples in the workspace by intent.](#display_intent_analysis)\n18. [Display the distribution of user utterances by intent using baseline vs. experiment workspaces](#display_intent_distribution_experiment)\n19. [Display a log comparison of utterances sent to baseline vs. experiment workspaces](#display_experiment_run_dataframe)\n20. [Summary and next steps](#summary)\n\n________", "cell_type": "markdown", "metadata": {}}, {"source": "## <a id=\"setup\"></a> Step 1. Install and import packages\n\nInstall and import the necessary packages.\n\n- <a href=\"https://github.com/watson-developer-cloud/python-sdk\" target=\"_blank\" rel=\"noopener no referrer\">watson-developer-cloud</a> is the Python SDK for Watson Developer Cloud services\n- <a href=\"https://docs.python.org/3/library/json.html\" target=\"_blank\" rel=\"noopener no referrer\">json</a> is a Python module used here for manipulating the raw data for workspaces and logs. \n- <a href=\"https://pandas.pydata.org/\" target=\"_blank\" rel=\"noopener no referrer\">pandas</a> is a Python library for processing and analyzing the baseline and experiment data.\n- <a href=\"https://docs.python.org/3/library/datetime.html\" target=\"_blank\" rel=\"noopener no referrer\">datetime</a> is a Python library used for naming the baseline and experiment files.\n- <a href=\"https://docs.python.org/3/library/time.html\" target=\"_blank\" rel=\"noopener no referrer\">time</a> is a Python library used for sleeping to limit rate limiting errors.\n- <a href=\"https://docs.python.org/3/library/re.html\" target=\"_blank\" rel=\"noopener no referrer\">re</a> is a Python library for regular expressions.", "cell_type": "markdown", "metadata": {}}, {"source": "!pip install --upgrade watson-developer-cloud", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "!pip install sklearn --upgrade", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#! pip install pixiedust\n#imports.... Run this each time after restarting the Kernel\nimport pixiedust #visualization\n\nimport json\nimport pandas\nimport datetime\nfrom watson_developer_cloud import AssistantV1\nimport time\nimport re ", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"credentials\"></a> Step 2. Prompt for Cloudant and Watson Assistant credentials \nWhen you run this, you will be prompted to paste in a JSON string of the form shown in the code block below.\n<br><i>Warning: Anyone with edit access to this Notebook will be able to read your credentials after you've run this cell.</i>", "cell_type": "markdown", "metadata": {}}, {"source": "import getpass  \nimport pandas as pd\n\ndata0 = {\n    \"assistant\": {\n        \"apikey\":\"<IAM_apiKey>\",\n        \"url\": \"<IAM_url>\",\n        \"prod_workspace\": \"<Workspace_ID prod>\",\n        \"test_workspace\": \"<Worskpace_ID test>\"        \n    },\n    \"cloudant\": {\n        \"username\": \"<Cloudant username>\",\n        \"password\": \"<Cloudant Password>\",\n        \"url\": \"<Cloudant URL>\"\n    }\n}\n\nVERSION = '2018-07-10'\ncredentials=pd.DataFrame(data0)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"cloudant\"></a> Step 3.  Create the Cloudant database\nHere we create the class that later functions will use.  \nBe sure to update the name of the Cloudant database.", "cell_type": "markdown", "metadata": {}}, {"source": "!pip install cloudant", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#Run this cell each time after restarting the Kernel\nfrom cloudant.client import Cloudant\n\nclass cloudant_database(object):\n    def __init__(self, db_name):\n        self.db_name = db_name\n        self.db = None\n        self.client = None\n\n    def login(self, username, password, url):\n        self.client = Cloudant(username, password, url=url, connect=True)\n\n    def create_database(self):\n        self.db = self.client.create_database(self.db_name)\n\n    def list_documents(self, pattern):\n        docs = []\n        for document in self.db:\n            name = document[\"_id\"]\n            \n            if name and re.match(pattern, name, re.I | re.M):\n                docs.append(document)\n        return docs\n\n    def store_json_log(self, data, doc_id):\n        try:\n            self.db[doc_id]\n        except:\n            self.db.create_document({\"_id\":doc_id, \"data\": data})\n            print('document added to Cloudant : {}'.format(doc_id))\n        else:\n            print('document exists')\n                      \ncdb = cloudant_database('conversation-analysis')\n\ncdb.login(credentials.cloudant.username,\n  credentials.cloudant.password,\n  credentials.cloudant.url);\n\ncdb.create_database()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"connect_assistant\"></a> Step 4. Connect to Watson Assistant\nBefore you run this, be sure you have two workspaces in your Watson Assistant instance:\n- One you want to treat as a baseline/production workspace, and \n- One you want to treat as your experiment workspace.\n\nUpdate the two workspace references with your workspace IDs.", "cell_type": "markdown", "metadata": {}}, {"source": "\n#create an SDK object for the baseline/production workspace (where to ask the questions to establish a true baseline)\nconversation = AssistantV1( iam_apikey=credentials.assistant.apikey, \n                            url=credentials.assistant.url,\n                            version=VERSION)\n\n# This is the baseline/production workspace\nconversation_prod_ws = credentials.assistant.prod_workspace\n\n\n#create an SDK object for the TEST workspace (where to ask the questions to establish true baseline)\nconversation_test = AssistantV1( iam_apikey=credentials.assistant.apikey, \n                                 url=credentials.assistant.url,\n                                 version=VERSION)\n#This is a experiment/test workspace\nconversation_test_ws = credentials.assistant.test_workspace\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"get_logs\"></a> Step 5. Get utterances from log data and store it in Cloudant for later use\nThis function calls Watson Assistant API to retrieve logs, using pagination if necessary.\nThe goal is to retrieve utterances (user inputs) from the logs.", "cell_type": "markdown", "metadata": {}}, {"source": "def get_logs(cursor, inputs, sdk_object, workspace_id, num_logs):\n    \n    page_limit = num_logs * 2 \n        \n    logs_response = sdk_object.list_logs(\n             workspace_id = workspace_id,\n             page_limit = num_logs,\n             cursor = cursor\n    ).get_result()\n    new_cursor = None\n\n    if 'pagination' in logs_response:\n        if 'next_url' in logs_response['pagination']:\n            next_url = logs_response['pagination']['next_url']\n            if next_url is not None:\n                index = next_url.find('?cursor=')\n                if index == -1:\n                    index = next_url.find('&cursor=')\n                if index != -1:\n                    next_url = next_url[index + 8:]\n                    index = next_url.find('&')\n                    if index != -1:\n                        next_url = next_url[:index]\n                    new_cursor = next_url\n    \n    \n    for log in logs_response[\"logs\"]:\n        try:\n            input = log['request']['input']['text'].lower()\n            if len(input) > 3: #we will (for now) ignore all utterances less than 3 chars long - need to revise\n                inputs.append(input)\n        except:\n            pass\n    inputs = list(set(inputs))\n    if len(inputs) >= num_logs:\n        inputs = inputs[:num_logs]\n        print(\"got {} user utterances via logs API\".format(len(inputs)) )\n        return inputs\n    \n    if new_cursor is not None:\n        time.sleep(0.05) #allow for a short break to try to limit rate limiting errors!\n        cursor, inputs, sdk_object, workspace_id, num_logs\n        inputs = get_logs(cursor=new_cursor, inputs = inputs, \n                          sdk_object = sdk_object, workspace_id = workspace_id, \n                          num_logs = num_logs)\n              \n    return inputs", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"send_questions\"></a> Step 6. Send experiment questions to test workspace \nThis function sends a set of questions captured previously from logs, extracts utterances and sends to Watson Assistant.\n\nCapture the responses and return that to the caller of this function. ", "cell_type": "markdown", "metadata": {}}, {"source": "def send_questions_and_get_workspace(conversation, workspace_id, logs, store_logs):\n    #for each utterance returned re-ask it against the 'test' workspace (the workspace against which you will be running the experiment)\n    responses = []\n    data = {}\n    i = 0\n    progress = 10;\n    if len(logs) > 50: \n        progress = 20\n    if len(logs) > 100:\n        progress = 25\n    if len(logs) > 500:\n        progress = 50\n        \n    for question in logs:\n    \n        i += 1\n        conversation_response = conversation.message(\n          workspace_id = workspace_id, \n            input = {'text': question}\n        ).get_result()\n        #Save the result in the form:\n        #{'text' : 'original question asked by user, retrieved from logs', \n        # 'intents':'the intents payload returned by the Watson Assistant API call',\n        # 'entities' : 'the entities payload returned by the conversation API call'}\n        intents = conversation_response['intents']\n        if len(intents) == 0:\n            intents = []\n            intents.append({\"intent\" : \"irrelevant\", \"confidence\" : 1.0})\n        result = {'text' : conversation_response['input']['text'], 'intents' : intents, 'entities' : conversation_response['entities']}\n        responses.append(result);\n        if i%progress == 0:\n            print(\"{} of {} questions processed..\".format(i, len(logs)))\n\n    #We have sent all the questions through the latest workspace - store the workspace also\n    print(\"All {} questions processed..\".format(i))\n    workspace_json = conversation.get_workspace(workspace_id=workspace_id, export=True).get_result()\n    data['results'] = responses\n    data['workspace'] = workspace_json\n    if store_logs == True:\n        data['logs'] = logs\n    return data", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"create_and_store_baseline\"></a> Step 7. Create and store baseline \nThis function retrieves logs and sends to another function to capture responses with a test workspace.\n\nStore the results in Cloudant in a new table prefixed with BL_ and including the current date and time for later analysis.\n", "cell_type": "markdown", "metadata": {}}, {"source": "def create_and_store_baseline(size):\n    logs = get_logs(cursor = None, inputs=[], \n            sdk_object = conversation, \n             workspace_id = conversation_prod_ws, \n             num_logs = size)\n\n    #send_questions_and_get_workspace(conversation, workspace_id, logs):\n    print(\"Sending questions to Watson Assistant....\")\n    document_to_store = send_questions_and_get_workspace(conversation = conversation, \n                                                         workspace_id = conversation_prod_ws, \n                                                         logs = logs, store_logs = True)\n    #Once all questions have been re-asked against the latest workspace store the results in Cloudant\n    timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n    name = \"BL_\" + timestamp\n    document_to_store[\"name\"] = name\n    \n    cdb.store_json_log(document_to_store, name) # store responses", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"get_baselines\"></a> Step 8. Get baselines from Cloudant \nThis function calls our Cloudant function to get documents that adhere to our naming convention for baseline documents.\n ", "cell_type": "markdown", "metadata": {}}, {"source": "def get_all_baselines () :\n    return cdb.list_documents(\"^BL_\\d{4}_\\d{2}_\\d{2}_\\d{6}$\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"run_experiment\"></a> Step 9. Define a function to run a specific experiment using baseline name\nThis function gets a baseline by name and runs the questions against a test workspace. ", "cell_type": "markdown", "metadata": {}}, {"source": "def run_experiment (baseline_name) :\n    baselines = get_all_baselines()\n    stored_baseline = None\n    document_to_store = None\n    for baseline in baselines:\n        if (baseline[\"_id\"] == baseline_name):\n            stored_baseline = baseline[\"data\"]\n            print (\"Found baseline: {}\".format(baseline_name))\n            if \"logs\" in stored_baseline:\n                print (\"Sending experiment questions to Watson Assistant...\")\n                logs = stored_baseline[\"logs\"]\n                document_to_store = send_questions_and_get_workspace(conversation = conversation_test, \n                                                                     workspace_id = conversation_test_ws, \n                                                                     logs = logs, store_logs = False)\n                return document_to_store\n            else:\n                print(\"LOGS WERE NOT STORED IN BASELINE.... EXITING!!!!\")\n            \n    return None       ", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"get_experiments_for_baseline\"></a> Step 10. Get previous experiments for a specific baseline \nThis function gets Cloudant experiment documents associated with a specific baseline.  ", "cell_type": "markdown", "metadata": {}}, {"source": "def get_experiments_for_baseline(baseline):\n    return cdb.list_documents(\"^\" + baseline + \"__EX__\\d{4}_\\d{2}_\\d{2}_\\d{6}$\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"compile_workspace_stats\"></a> Step 11. Compute and compile stats for a workspace \nThis function calculates stats about intents, entities (including values and synonyms) and dialog notes.\n", "cell_type": "markdown", "metadata": {}}, {"source": "def compile_workspace_stats(workspace_json):\n    intents = workspace_json['intents']\n    entities = workspace_json['entities']\n    #print(json.dumps(intents, indent=4, sort_keys=True))\n    #print(json.dumps(entities, indent=1))\n    \n    intents_len =  len(intents)\n    dialog_nodes_len = len(workspace_json['dialog_nodes'])\n    entities_len = len(entities)\n\n    # print(intents_len, entities_len)\n    examples_len = 0\n    intent_info = [];\n\n    for intent in intents:\n        intent_info.append({'intent' : intent['intent'], 'len' : len(intent['examples'])})\n        examples_len += len(intent['examples'])\n\n    values_len = 0\n    synonyms_len = 0\n    values_len = 0\n\n    for entity0 in entities:\n      values_len += len(entity0['values'])\n      for value0 in entity0['values']:\n        try:    \n            synonyms_len += len(value0['synonyms'])\n           #     synonyms_len += (len(value['synonyms']))\n        #add the exception for the entity type Pattern\n        except Exception as exc:\n            pass\n            \n    stats = {}\n    stats['ws_stats'] = [{'name' : 'entities', 'len' : entities_len},\n    {'name' : 'values', 'len' : values_len},\n    {'name' : 'synonyms', \"len\" : synonyms_len},\n    {'name' : 'intents', 'len' : intents_len},\n    {'name' : 'examples', 'len' : examples_len},\n    {'name' : 'dialog_nodes', 'len' : dialog_nodes_len}]\n\n\n    return stats, intent_info", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"call_create_store_baseline\"></a> Step 12. Call the function to create and store the baseline\nCalls function, passing in max number of utterances.\n\n\n**Note:** This assumes that you have log data available for the workspace you've set as `conversation_prod_ws`.", "cell_type": "markdown", "metadata": {}}, {"source": "create_and_store_baseline(size = 400)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"get_baseline_run_experiment\"></a> Step 13. Call function to run experiment and store results \nPick one baseline and run experiment with that.\n ", "cell_type": "markdown", "metadata": {}}, {"source": "baselines = get_all_baselines()\nbaseline_name = baselines[len(baselines) - 1][\"_id\"]\nexperiment = run_experiment(baseline_name = baseline_name)\n\n# store experiment result with convention of using the baseline name and the experiment date time\nname = baseline_name + \"__EX__\" + datetime.datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\nexperiment[\"name\"] = name\ncdb.store_json_log(experiment, name) # store experiment", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"call_compile_workspace_stats\"></a> Step 14. Compile workspace stats for baseline and experiment\nGets stats for the workspace associated with last baseline and last experiment.\n ", "cell_type": "markdown", "metadata": {}}, {"source": "baselines = get_all_baselines()\nbaseline_name = baselines[len(baselines) - 1][\"_id\"]\nworkspace = baselines[len(baselines) - 1][\"data\"][\"workspace\"]\nbase_ws_stats = compile_workspace_stats(workspace_json=workspace)\n\nexperiments = get_experiments_for_baseline(baseline=baseline_name)\nworkspace = experiments[len(experiments) - 1][\"data\"][\"workspace\"]\nexp_ws_stats = compile_workspace_stats(workspace_json=workspace)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"prepare_frames_display\"></a> Step 15. Prepare workspace analysis data and display the intent distribution\n- Create an intent distribution of the results from the baseline and the experiment (and display the comparison).\n- Create another data frame for analysis of intents in each workspace, counting the number of examples for each intent.\n- Create another data frame for more complete analysis of each workspace including entities.\n", "cell_type": "markdown", "metadata": {}}, {"source": "import pandas\n\n#count how many times each intent was used per experiment run\ncount = {}\n\nbaselines = get_all_baselines()\nbaseline_name = baselines[len(baselines) - 1][\"_id\"]\nprod_ws = baselines[len(baselines) - 1][\"data\"][\"workspace\"]\nbaseline = baselines[len(baselines) - 1][\"data\"][\"results\"]\n\nexperiments = get_experiments_for_baseline(baseline=baseline_name)\nexperiment_name = experiments[len(experiments) - 1][\"_id\"]\ntest_ws = experiments[len(experiments) - 1][\"data\"][\"workspace\"]\nexperiment_results = experiments[len(experiments) - 1][\"data\"][\"results\"]\n\nfor row in baseline:\n    name = row['intents'][0]['intent']\n    if name not in count:\n        count[name] = {'ex1' : 0, 'ex2' : 0}\n    count[name]['ex1'] = count[name]['ex1'] + 1\n\nfor row in experiment_results:\n    name = row['intents'][0]['intent']\n    \n    if name not in count:\n        count[name] = {'ex1' : 0, 'ex2' : 0}\n        \n    count[name]['ex2'] = count[name]['ex2'] + 1   \n    \narr = []    \nfor key in count:\n    arr.append({'intent name' : key, 'baseline' : count[key]['ex1'], 'experiment' : count[key]['ex2']})\n    \nintent_distribution_df = pandas.DataFrame(arr)\n\nws_analysis = [] \nws_analysis.append({'workspace component' : 'intents', 'baseline' : len(prod_ws['intents']), 'experiment' : len(test_ws['intents'])})\nws_analysis.append({'workspace component' : 'entities', 'baseline' : len(prod_ws['entities']), 'experiment' : len(test_ws['entities'])})\nws_analysis.append({'workspace component' : 'dialog_nodes', 'baseline' : len(prod_ws['dialog_nodes']), 'experiment' : len(test_ws['dialog_nodes'])})\nws_analysis.append({'workspace component' : 'counterexamples', 'baseline' : len(prod_ws['counterexamples']), 'experiment' : len(test_ws['counterexamples'])})\nprod_examples = 0\n\nintent_analysis = []\nintent_analysis.append({'intent' : 'irrelevant', 'baseline': len(prod_ws['counterexamples']), 'experiment' : len(test_ws['counterexamples'])})\n\nfor intent in prod_ws['intents']:\n    prod_examples = prod_examples + len(intent['examples'])\n    intent_analysis.append({'intent' : intent['intent'], 'baseline' : len(intent['examples']), 'experiment' : 0})\n    \ntest_examples = 0\nfor intent in test_ws['intents']:\n    test_examples = test_examples + len(intent['examples'])\n    found = False\n    for intnt in intent_analysis:\n        if intnt['intent'] == intent['intent']:\n            found = True\n            intnt['experiment'] = len(intent['examples'])\n            break\n                                      \n    if not found: \n        intent_analysis.append({'intent' : intent['intent'], 'baseline' : 0, 'experiment' : len(intent['examples'])})\n \nws_analysis.append({'workspace component' : 'examples', 'baseline' : prod_examples, 'experiment' : test_examples})\n\nprod_values = 0\nprod_synonyms = 0\nfor entity in prod_ws['entities']:\n    prod_values = prod_values + len(entity['values'])\n    for value in entity['values']:\n        try:    \n            prod_synonyms = prod_synonyms + len(value['synonyms'])\n        #add the exception for the entity type Pattern\n        except Exception as exc:\n            pass\n        \ntest_values = 0\ntest_synonyms = 0\nfor entity in test_ws['entities']:\n    test_values = test_values + len(entity['values'])\n    for value in entity['values']:     \n        try:    \n            test_synonyms = test_synonyms + len(value['synonyms'])\n        #add the exception for the entity type Pattern\n        except Exception as exc:\n            pass   \n\nws_analysis.append({'workspace component' : 'values', 'baseline' : prod_values, 'experiment' : test_values})\nws_analysis.append({'workspace component' : 'synonyms', 'baseline' : prod_synonyms, 'experiment' : test_synonyms})\n\nws_analysis_df = pandas.DataFrame(ws_analysis)\nintent_analysis_df = pandas.DataFrame(intent_analysis)\n\n# make a data frame with the original utterance, the intent from each run, \n# the confidence, whether or not the intent changed, and the confidence delta\nmerged1 = [{\"text\": b[\"text\"], \"intent_1\" : b['intents'][0]['intent'], \n           \"conf_1\" : b['intents'][0]['confidence']} for b in baseline]\nmerged2 =[{\"text\": b[\"text\"], \"intent_2\" : b['intents'][0]['intent'], \n           \"conf_2\" : b['intents'][0]['confidence']} for b in experiment_results]\nmerged = list(zip(merged1, merged2))\n\nmerged = [{'intent_1': b[0]['intent_1'], 'conf_1': b[0]['conf_1'],\n          'intent_2': b[1]['intent_2'], 'conf_2': b[1]['conf_2'], \n          'intent_changed' : b[0]['intent_1'] != b[1]['intent_2'],\n          'conf_delta' : b[1]['conf_2'] - b[0]['conf_1'], 'text' : b[0]['text']} for b in merged]\n\nexperiment_run_dataframe = pandas.DataFrame(merged)\n\n# Code below filters out all information except for cases where the intent had been changed between baseline and experiment.\n# For now let's keep it all for display.\n# experiment_run_dataframe = experiment_run_dataframe.loc[experiment_run_dataframe['intent_changed']==True]\n\n#pdf = pdf.loc[pdf['conf_delta']>0.1]\n#display(pdf)", "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {"orientation": "horizontal", "lineChartType": "grouped", "chartsize": "100", "mpld3": "false", "aggregation": "SUM", "rowCount": "500", "handlerId": "dataframe", "valueFields": "baseline,experiment", "rendererId": "matplotlib", "sortby": "Values DESC", "timeseries": "false", "charttype": "grouped", "keyFields": "name", "legend": "true"}}}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"display_ws_analysis\"></a> Step 16. Compare the baseline vs. experiment workspaces \nCompare aspects of baseline vs. experiment workspace. ", "cell_type": "markdown", "metadata": {}}, {"source": "ws_analysis_df", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "display(ws_analysis_df)", "cell_type": "code", "metadata": {"scrolled": true, "pixiedust": {"displayParams": {"rowCount": "500", "handlerId": "barChart", "valueFields": "baseline,experiment", "rendererId": "bokeh", "chartsize": "83", "keyFields": "workspace component", "aggregation": "SUM"}}}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"display_intent_analysis\"></a> Step 17. Display number of training examples in the workspace by intent\n Intent analysis", "cell_type": "markdown", "metadata": {}}, {"source": "display(intent_analysis_df)", "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {"orientation": "horizontal", "stretch": "false", "chartsize": "91", "mpld3": "false", "aggregation": "SUM", "rowCount": "500", "handlerId": "barChart", "valueFields": "baseline,experiment", "rendererId": "matplotlib", "sortby": "Values ASC", "charttype": "stacked", "keyFields": "intent", "legend": "false"}}}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"display_intent_distribution_experiment\"></a> Step 18.  Display the distribution of user utterances by intent using baseline vs. experiment workspaces", "cell_type": "markdown", "metadata": {}}, {"source": "# Display the distribution of user utterances by intent using baseline vs. experiment workspaces\ndisplay(intent_distribution_df)", "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {"orientation": "horizontal", "stretch": "true", "chartsize": "73", "aggregation": "SUM", "rowCount": "500", "handlerId": "barChart", "valueFields": "experiment,baseline", "rendererId": "matplotlib", "keyFields": "intent name", "legend": "true"}}}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"display_experiment_run_dataframe\"></a> Step 19.  Display a log comparison of utterances sent to baseline vs. experiment workspaces", "cell_type": "markdown", "metadata": {}}, {"source": "display(experiment_run_dataframe)", "cell_type": "code", "metadata": {"scrolled": true, "pixiedust": {"displayParams": {"handlerId": "tableView"}}}, "outputs": [], "execution_count": null}, {"source": "## <a id=\"summary\"></a> Summary and next steps\nYou've learned how to use the Watson Assistant API to perform workspace analysis and track experiments against a baseline.\nTry using your own workspace and log data to make improvements for your Watson Assistant application!\n\n\nLearn more:\n- <a href=\"https://www.ibm.com/watson/developercloud/conversation/api/v1/\" target=\"_blank\" rel=\"noopener noreferrer\">Watson Assistant API reference</a>\n- <a href=\"https://github.com/watson-developer-cloud/python-sdk\" target=\"_blank\" rel=\"noopener noreferrer\">Watson Assistant Python SDK</a>", "cell_type": "markdown", "metadata": {}}, {"source": "Copyright \u00a9 2018 IBM. This notebook and its source code are released under the terms of the MIT License.", "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}